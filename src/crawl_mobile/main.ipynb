{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//Crawl data mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://www.gsmarena.com/apple_iphone_5s-5685.php\n",
      "Processing https://www.gsmarena.com/samsung_galaxy_s23-12082.php\n",
      "Processing https://www.gsmarena.com/samsung_galaxy_f14_4g-13247.php\n",
      "Processing https://www.gsmarena.com/apple_watch_series_7_aluminum-11107.php\n",
      "Processing https://www.gsmarena.com/samsung_galaxy_z_flip3_5g-11044.php\n"
     ]
    }
   ],
   "source": [
    "pages = {\n",
    "    'Apple':[\n",
    "        'https://www.gsmarena.com/apple-phones-48.php',\n",
    "        'https://www.gsmarena.com/apple-phones-f-48-0-p2.php',\n",
    "        'https://www.gsmarena.com/apple-phones-f-48-0-p3.php',\n",
    "    ],\n",
    "    'Samsung':[\n",
    "        'https://www.gsmarena.com/samsung-phones-9.php',\n",
    "        'https://www.gsmarena.com/samsung-phones-f-9-0-p2.php',\n",
    "        'https://www.gsmarena.com/samsung-phones-f-9-0-p3.php'\n",
    "    ],\n",
    "    'Huawei':[\n",
    "        'https://www.gsmarena.com/huawei-phones-58.php',\n",
    "        'https://www.gsmarena.com/huawei-phones-f-58-0-p2.php',\n",
    "        'https://www.gsmarena.com/huawei-phones-f-58-0-p3.php'\n",
    "    ],\n",
    "    'LG':[\n",
    "        'https://www.gsmarena.com/lg-phones-20.php',\n",
    "        'https://www.gsmarena.com/lg-phones-f-20-0-p2.php',\n",
    "        'https://www.gsmarena.com/lg-phones-f-20-0-p3.php'\n",
    "    ],\n",
    "    'Nokia':[\n",
    "        'https://www.gsmarena.com/nokia-phones-1.php',\n",
    "        'https://www.gsmarena.com/nokia-phones-f-1-0-p2.php',\n",
    "        'https://www.gsmarena.com/nokia-phones-f-1-0-p3.php'\n",
    "    ],\n",
    "    'Xiaomi':[\n",
    "        'https://www.gsmarena.com/xiaomi-phones-80.php',\n",
    "        'https://www.gsmarena.com/xiaomi-phones-f-80-0-p2.php',\n",
    "        'https://www.gsmarena.com/xiaomi-phones-f-80-0-p3.php'\n",
    "    ],\n",
    "    'Google':[\n",
    "        'https://www.gsmarena.com/google-phones-107.php'\n",
    "        ]\n",
    "}\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Read existing product names to skip those already processed\n",
    "existing_products = set()\n",
    "json_file = \"output_data.json\"\n",
    "existing_data = []\n",
    "\n",
    "# Load existing data from the JSON file if it exists\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, mode='r', encoding='utf-8') as file:\n",
    "        existing_data = json.load(file)\n",
    "        for item in existing_data:\n",
    "            product_name = item.get(\"PRODUCT_NAME\")\n",
    "            if product_name:\n",
    "                existing_products.add(product_name)\n",
    "\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# Define a function to process each URL\n",
    "def process_url(data, temp_file_path):\n",
    "    branch, url = data\n",
    "    driver = create_driver()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    all_data = []  # Initialize an empty list to store all data objects\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        review_body = wait.until(EC.presence_of_element_located((By.ID, \"review-body\")))\n",
    "        links = wait.until(lambda driver: review_body.find_elements(By.TAG_NAME, \"a\")) \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page {url}\")\n",
    "        driver.quit()\n",
    "        return\n",
    "    product_hrefs = []\n",
    "    for link in links:\n",
    "        # Chờ để tìm thấy phần tử có thẻ strong chứa tên sản phẩm\n",
    "        product_name = wait.until(lambda driver: link.find_element(By.TAG_NAME, \"strong\")).text\n",
    "        # Lấy href từ link\n",
    "        href = link.get_attribute(\"href\")\n",
    "        # Thêm cặp product_name và href vào danh sách\n",
    "        product_hrefs.append((product_name, href))\n",
    "    for product_name, href in product_hrefs:\n",
    "        try:\n",
    "            if product_name in existing_products:\n",
    "                continue\n",
    "            print(href)\n",
    "            driver.get(href)\n",
    "            specs_list = wait.until(lambda driver: driver.find_element(By.ID, \"specs-list\"))\n",
    "            tables = wait.until(lambda driver: specs_list.find_elements(By.TAG_NAME, \"table\"))\n",
    "            data = {\"PRODUCT_NAME\": product_name, \"BRANCH\": branch}\n",
    "\n",
    "            for table in tables:\n",
    "                key = wait.until(lambda driver: table.find_element(By.TAG_NAME, \"th\"))\n",
    "                tr_list = wait.until(lambda driver: table.find_elements(By.TAG_NAME, \"tr\"))\n",
    "                master_key = key.text\n",
    "                sub_data = {}\n",
    "                previous_subkey = ''\n",
    "                for tr in tr_list:\n",
    "                    if tr.text != \"\":\n",
    "                        td_list = wait.until(lambda driver: tr.find_elements(By.TAG_NAME, \"td\"))\n",
    "                        subkey = td_list[0].text\n",
    "                        subvalue = td_list[1].text\n",
    "                        if subkey == '' or subkey == \" \":\n",
    "                            if master_key.lower() == 'network':\n",
    "                                sub_data[previous_subkey] = sub_data[previous_subkey] + \";\" + subvalue\n",
    "                            else:\n",
    "                                sub_data[\"OTHER\"] = subvalue\n",
    "                        else:\n",
    "                            sub_data[subkey] = subvalue\n",
    "                            previous_subkey = subkey\n",
    "                        data[master_key] = sub_data\n",
    "\n",
    "            all_data.append(data)  # Add data object to all_data list\n",
    "            time.sleep(10)\n",
    "            print(f\"Success Link :{href}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for link {link}\")\n",
    "            continue\n",
    "\n",
    "    # Save all_data to the temporary file\n",
    "    with open(temp_file_path, \"w\", encoding=\"utf-8\") as temp_file:\n",
    "        json.dump(all_data, temp_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Flatten the URLs with their corresponding branches\n",
    "urls_with_branches = [(branch, url) for branch, urls in pages.items() for url in urls]\n",
    "# Create a list to store paths of temporary files for each thread\n",
    "temp_files = []\n",
    "\n",
    "# Run each (branch, url) tuple in a separate thread\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Adjust max_workers as needed\n",
    "    futures = []\n",
    "    for data in urls_with_branches:\n",
    "        # Create a temporary file for each thread\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\", mode=\"w\", encoding=\"utf-8\")\n",
    "        temp_files.append(temp_file.name)\n",
    "        # Submit the process_url function with temp_file.name\n",
    "        futures.append(executor.submit(process_url, data, temp_file.name))\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for future in as_completed(futures):\n",
    "        future.result()\n",
    "\n",
    "# Combine existing data and data from all temporary JSON files into one\n",
    "\n",
    "combined_data = existing_data\n",
    "for temp_file_path in temp_files:\n",
    "    with open(temp_file_path, \"r\", encoding=\"utf-8\") as temp_file:\n",
    "        temp_data = json.load(temp_file)\n",
    "        combined_data.extend(temp_data)\n",
    "    # Remove the temporary file after reading\n",
    "    os.remove(temp_file_path)\n",
    "\n",
    "# Write combined data to the final JSON file\n",
    "with open(\"output_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(combined_data, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl CPU Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to model_architecture.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content from a file\n",
    "with open('./Smartphone Processors - Benchmark List - NotebookCheck.net Tech.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Locate the table with the specific ID\n",
    "table = soup.find('div', {'id':'c2052256'})\n",
    "\n",
    "# Dictionary to store Model as key and Architecture as value\n",
    "data_dict = {}\n",
    "\n",
    "# Check if the table is found\n",
    "if table:\n",
    "    # Find the headers to locate the index of 'Model' and 'Architecture'\n",
    "    trs = table.find_all('tr')\n",
    "    \n",
    "    for tr in trs:\n",
    "        tds = tr.find_all('td')\n",
    "        if tds[1].get_text() != \"Model\":\n",
    "            pro = {tds[1].get_text() : tds[2].get_text()}\n",
    "            data_dict[tds[1].get_text() ] =  tds[2].get_text()\n",
    "\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('model_architecture.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Data saved to model_architecture.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to model_architecture.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content from a file\n",
    "with open('./iOS version history - Wikipedia.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Locate the table with the specific ID\n",
    "table = soup.find('table', {'class':'wikitable'})\n",
    "\n",
    "# Dictionary to store Model as key and Architecture as value\n",
    "data_dict = {}\n",
    "\n",
    "# Check if the table is found\n",
    "if table:\n",
    "    # Find the headers to locate the index of 'Model' and 'Architecture'\n",
    "    trs = table.find_all('tr')\n",
    "    \n",
    "    for tr in trs:\n",
    "        tds = tr.find_all('td')\n",
    "        ths = tr.find_all('th')\n",
    "        if len(tds) > 0 and len(ths) > 0:\n",
    "            data_dict[ths[0].get_text().replace(\"\\n\",\"\")] =  tds[0].get_text().replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('os_info.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Data saved to model_architecture.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_os_info(link, version_order, release_der, output, tableorder):\n",
    "    # Load the HTML content from a file\n",
    "    with open(link, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Locate the table with the specific ID\n",
    "    table = soup.find_all('table', {'class':'wikitable'})[tableorder]\n",
    "\n",
    "    # Dictionary to store Model as key and Architecture as value\n",
    "    data_dict = {}\n",
    "\n",
    "    # Check if the table is found\n",
    "    if table:\n",
    "        # Find the headers to locate the index of 'Model' and 'Architecture'\n",
    "        trs = table.find_all('tr')\n",
    "        \n",
    "        for tr in trs:\n",
    "            tds = tr.find_all('td')\n",
    "            ths = tr.find_all('th')\n",
    "            if len(tds) > 3:\n",
    "                data_dict[(\"Android \" + tds[version_order].get_text()).replace(\"\\n\",\"\")] =  tds[release_der].get_text().replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "    # Save the data to a JSON file\n",
    "    with open(f\"./os/{output}\", 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Data saved to model_architecture.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to model_architecture.json\n"
     ]
    }
   ],
   "source": [
    "# HarmonyOS\n",
    "get_os_info('./HarmonyOS version history - Wikipedia.html', 0, 3, 'os_info_har.json',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_and_format_date(text):\n",
    "    # Sử dụng biểu thức chính quy để trích xuất đoạn có năm, tháng và ngày\n",
    "    match = re.search(r'(\\d{4}), (\\w+) (\\d{1,2})', text)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month_name = match.group(2)\n",
    "        day = match.group(3)\n",
    "\n",
    "        # Chuyển đổi tên tháng thành số\n",
    "        date_str = f\"{year} {month_name} {day}\"\n",
    "        date_obj = datetime.strptime(date_str, \"%Y %B %d\")\n",
    "\n",
    "        # Định dạng lại ngày tháng\n",
    "        formatted_date = date_obj.strftime(\"%Y/%m/%d\")\n",
    "        return formatted_date\n",
    "    else:\n",
    "        return \"Không tìm thấy ngày tháng trong chuỗi.\"\n",
    "\n",
    "\n",
    "def split_storage_and_ram(text):\n",
    "    # Kiểm tra nếu text là None\n",
    "    if text is None:\n",
    "        return None, None\n",
    "\n",
    "    # Sử dụng regex để tìm tất cả dung lượng lưu trữ và RAM\n",
    "    storage = re.findall(r'\\d+(?:GB|TB|MB)', text)\n",
    "    ram = re.findall(r'\\d+(?:GB|MB) RAM', text)\n",
    "    \n",
    "    # Tạo chuỗi từ danh sách dung lượng lưu trữ\n",
    "    storage_str = \", \".join(storage) if storage else None\n",
    "    \n",
    "    # Kiểm tra các phần tử RAM có giống nhau không, nếu có phần tử\n",
    "    if ram:\n",
    "        # Xử lý chuỗi RAM bằng cách loại bỏ ' RAM' từ kết quả\n",
    "        ram = [r.replace(' RAM', '') for r in ram]\n",
    "        \n",
    "        if all(ram[0] == r for r in ram):\n",
    "            ram_str = ram[0]  # Lấy phần tử đầu tiên nếu tất cả giống nhau\n",
    "        else:\n",
    "            ram_str = \", \".join(ram)  # Lấy tất cả nếu không giống nhau\n",
    "    else:\n",
    "        ram_str = None  # Nếu không có RAM thì trả về None\n",
    "\n",
    "    return storage_str, ram_str\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    # Remove text inside parentheses, including the parentheses\n",
    "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "def get_os(text):\n",
    "    # Tách chuỗi bằng dấu phẩy\n",
    "    parts = text.split(\", \")\n",
    "    return parts[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON files\n",
    "with open('./output_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "with open('./model_architecture.json', 'r') as file:\n",
    "    arc_data = json.load(file)\n",
    "with open('./os/os_info.json', 'r') as file:\n",
    "    os_data = json.load(file)\n",
    "with open('./company_info.json', 'r') as file:\n",
    "    company_data = json.load(file)\n",
    "\n",
    "stg_data = []\n",
    "for item in data:\n",
    "    if \"Watch\" in item.get('PRODUCT_NAME', \"\"):\n",
    "        continue\n",
    "    storage_str, ram_str = split_storage_and_ram(item.get('MEMORY', {}).get('Internal', None))\n",
    "    data_dict = {\n",
    "        'name': item.get('PRODUCT_NAME', None),\n",
    "        'releaseDate': extract_and_format_date(item.get('LAUNCH', {}).get('Status', None)),\n",
    "        'simType': item.get('NETWORK', {}).get('Technology', None),\n",
    "        'storageCapacity': storage_str,\n",
    "        'ramSize': ram_str,\n",
    "        'Screen': {\n",
    "            'type': item.get('DISPLAY', {}).get('Type', None),\n",
    "            'size': item.get('DISPLAY', {}).get('Size', None),\n",
    "            'resolution': item.get('DISPLAY', {}).get('Resolution', None)\n",
    "        },\n",
    "        'CPU': {\n",
    "            'name': item.get('PLATFORM', {}).get('Chipset', None),\n",
    "            'parameter': item.get('PLATFORM', {}).get('CPU', None),\n",
    "            'arch': arc_data.get(remove_parentheses(item.get('PLATFORM', {}).get('Chipset', '')), None)\n",
    "        },\n",
    "        'Battery': {\n",
    "            'capacity': item.get('BATTERY', {}).get('Type', '').split(',')[0] if item.get('BATTERY', {}).get('Type') else None,\n",
    "            'charge': item.get('BATTERY', {}).get('Charging', None)\n",
    "        },\n",
    "        'OS': {\n",
    "            'name': item.get('PLATFORM', {}).get('OS', None),\n",
    "            'releaseDate': os_data.get(remove_parentheses(get_os(item.get('PLATFORM', {}).get('OS', ''))), None)\n",
    "        },\n",
    "        'FrontCamera': {\n",
    "            'type': next(iter(item.get(\"SELFIE CAMERA\", {})), None),\n",
    "            'resolution': item.get(\"SELFIE CAMERA\", {}).get(next(iter(item.get(\"SELFIE CAMERA\", {})), ''), None),\n",
    "            'video': item.get(\"SELFIE CAMERA\", {}).get('Video', None),\n",
    "            'features': item.get(\"SELFIE CAMERA\", {}).get('Features', None)\n",
    "        },\n",
    "        'MainCamera': {\n",
    "            'type': next(iter(item.get(\"MAIN CAMERA\", {})), None),\n",
    "            'resolution': item.get(\"MAIN CAMERA\", {}).get(next(iter(item.get(\"MAIN CAMERA\", {})), ''), None),\n",
    "            'video': item.get(\"MAIN CAMERA\", {}).get('Video', None),\n",
    "            'features': item.get(\"MAIN CAMERA\", {}).get('Features', None)\n",
    "        },\n",
    "        'Brand': {\n",
    "            'name': item.get('BRANCH', None),\n",
    "            'country': company_data.get(item.get('BRANCH', ''), {}).get(\"country\", None),\n",
    "            'foundDate': company_data.get(item.get('BRANCH', ''), {}).get(\"foundDate\", None)\n",
    "        }\n",
    "    }\n",
    "    stg_data.append(data_dict)\n",
    "with open(f\"./data_ontology_stg.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(stg_data, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF data has been successfully generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import XSD\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the namespace for your ontology\n",
    "SMP = Namespace(\"http://www.semanticweb.org/hust/master/2024/10/ontologies/smartphone.owl#\")\n",
    "\n",
    "# Function to clean strings for URI creation\n",
    "def clean_string_for_uri(value):\n",
    "    return re.sub(r'[^\\w]', '_', value).lower()\n",
    "\n",
    "# Function to convert date formats to YYYY-MM-DD\n",
    "def format_date(date_str):\n",
    "    try:\n",
    "        if '/' in date_str:\n",
    "            return datetime.strptime(date_str, \"%Y/%m/%d\").strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            return datetime.strptime(date_str, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open('data_ontology_stg.json', 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Create the graph\n",
    "g = Graph()\n",
    "g.bind(\"smp\", SMP)\n",
    "\n",
    "# Cache for shared URIs to reuse\n",
    "uri_cache = {}\n",
    "\n",
    "def get_or_create_shared_uri(prop_type, value):\n",
    "    if not value:  # Check if value is None or empty\n",
    "        return None\n",
    "    clean_value = clean_string_for_uri(value)\n",
    "    uri_key = f\"{prop_type}_{clean_value}\"\n",
    "    \n",
    "    if uri_key not in uri_cache:\n",
    "        uri_cache[uri_key] = SMP[clean_value]\n",
    "    \n",
    "    return uri_cache[uri_key]\n",
    "\n",
    "# Iterate over each smartphone object in the list\n",
    "for smartphone in json_data:\n",
    "    product_name_clean = clean_string_for_uri(smartphone[\"name\"])\n",
    "    product_uri = SMP[product_name_clean]\n",
    "\n",
    "    # Add main smartphone attributes\n",
    "    g.add((product_uri, RDF.type, SMP.SmartPhone))\n",
    "    g.add((product_uri, SMP.name, Literal(smartphone[\"name\"], datatype=XSD.string)))\n",
    "\n",
    "    # Handle release date\n",
    "    if smartphone[\"releaseDate\"]:\n",
    "        release_date = format_date(smartphone[\"releaseDate\"])\n",
    "        if release_date:\n",
    "            g.add((product_uri, SMP.releaseDate, Literal(release_date, datatype=XSD.date)))\n",
    "\n",
    "    # Add simType and storageCapacity\n",
    "    g.add((product_uri, SMP.simType, Literal(smartphone[\"simType\"], datatype=XSD.string)))\n",
    "    g.add((product_uri, SMP.storageCapacity, Literal(smartphone[\"storageCapacity\"], datatype=XSD.string)))\n",
    "    g.add((product_uri, SMP.ramSize, Literal(smartphone[\"ramSize\"], datatype=XSD.string)))\n",
    "\n",
    "    # Handle Screen properties\n",
    "    if \"Screen\" in smartphone:\n",
    "        screen_uri = SMP[product_name_clean + \"_screen\"]\n",
    "        g.add((screen_uri, RDF.type, SMP.Screen))\n",
    "        g.add((screen_uri, SMP.type, Literal(smartphone[\"Screen\"][\"type\"], datatype=XSD.string)))\n",
    "        g.add((screen_uri, SMP.size, Literal(smartphone[\"Screen\"][\"size\"], datatype=XSD.string)))\n",
    "        g.add((screen_uri, SMP.resolution, Literal(smartphone[\"Screen\"][\"resolution\"], datatype=XSD.string)))\n",
    "        g.add((product_uri, SMP.hasScreen, screen_uri))\n",
    "\n",
    "    # Handle CPU properties\n",
    "    if \"CPU\" in smartphone and smartphone[\"CPU\"][\"name\"]:\n",
    "        cpu_name = smartphone[\"CPU\"][\"name\"]\n",
    "        cpu_uri = get_or_create_shared_uri(\"cpu\", cpu_name)\n",
    "        if cpu_uri:  # Check if uri is not None\n",
    "            g.add((cpu_uri, RDF.type, SMP.CPU))\n",
    "            g.add((cpu_uri, SMP.name, Literal(cpu_name, datatype=XSD.string)))\n",
    "            g.add((cpu_uri, SMP.parameter, Literal(smartphone[\"CPU\"][\"parameter\"], datatype=XSD.string)))\n",
    "            g.add((cpu_uri, SMP.arch, Literal(smartphone[\"CPU\"][\"arch\"], datatype=XSD.string)))\n",
    "            g.add((product_uri, SMP.hasCPU, cpu_uri))\n",
    "\n",
    "    # Handle Battery properties (which may be unique per product)\n",
    "    if \"Battery\" in smartphone:\n",
    "        battery_uri = SMP[product_name_clean + \"_battery\"]\n",
    "        g.add((battery_uri, RDF.type, SMP.Battery))\n",
    "        g.add((battery_uri, SMP.capacity, Literal(smartphone[\"Battery\"][\"capacity\"], datatype=XSD.string)))\n",
    "        g.add((product_uri, SMP.hasBattery, battery_uri))\n",
    "\n",
    "    # Handle OS properties\n",
    "    if \"OS\" in smartphone and smartphone[\"OS\"][\"name\"]:\n",
    "        os_name = smartphone[\"OS\"][\"name\"]\n",
    "        os_uri = get_or_create_shared_uri(\"os\", os_name)\n",
    "        if os_uri:  # Check if uri is not None\n",
    "            g.add((os_uri, RDF.type, SMP.OS))\n",
    "            g.add((os_uri, SMP.name, Literal(os_name, datatype=XSD.string)))\n",
    "            if smartphone[\"OS\"][\"releaseDate\"]:\n",
    "                os_release_date = format_date(smartphone[\"OS\"][\"releaseDate\"])\n",
    "                if os_release_date:\n",
    "                    g.add((os_uri, SMP.releaseDate, Literal(os_release_date, datatype=XSD.date)))\n",
    "            g.add((product_uri, SMP.hasOS, os_uri))\n",
    "\n",
    "    # Handle Camera properties (Front and Main)\n",
    "    for camera_type in [\"FrontCamera\", \"MainCamera\"]:\n",
    "        if camera_type in smartphone:\n",
    "            camera_uri = SMP[product_name_clean + \"_\" + camera_type.lower()]\n",
    "            camera_class = SMP.FrontCamera if camera_type == \"FrontCamera\" else SMP.MainCamera\n",
    "            g.add((camera_uri, RDF.type, camera_class))\n",
    "            g.add((camera_uri, SMP.type, Literal(smartphone[camera_type][\"type\"], datatype=XSD.string)))\n",
    "            g.add((camera_uri, SMP.resolution, Literal(smartphone[camera_type][\"resolution\"], datatype=XSD.string)))\n",
    "            g.add((camera_uri, SMP.video, Literal(smartphone[camera_type][\"video\"], datatype=XSD.string)))\n",
    "            g.add((camera_uri, SMP.features, Literal(smartphone[camera_type][\"features\"], datatype=XSD.string)))\n",
    "            g.add((product_uri, SMP.hasCamera, camera_uri))\n",
    "\n",
    "    # Handle Brand properties\n",
    "    if \"Brand\" in smartphone and smartphone[\"Brand\"][\"name\"]:\n",
    "        brand_name = smartphone[\"Brand\"][\"name\"]\n",
    "        brand_uri = get_or_create_shared_uri(\"brand\", brand_name)\n",
    "        if brand_uri:  # Check if uri is not None\n",
    "            g.add((brand_uri, RDF.type, SMP.Brand))\n",
    "            g.add((brand_uri, SMP.name, Literal(brand_name, datatype=XSD.string)))\n",
    "            g.add((brand_uri, SMP.country, Literal(smartphone[\"Brand\"][\"country\"], datatype=XSD.string)))\n",
    "            if smartphone[\"Brand\"][\"foundDate\"]:\n",
    "                brand_found_date = format_date(smartphone[\"Brand\"][\"foundDate\"])\n",
    "                if brand_found_date:\n",
    "                    g.add((brand_uri, SMP.foundDate, Literal(brand_found_date, datatype=XSD.date)))\n",
    "            g.add((product_uri, SMP.hasBrand, brand_uri))\n",
    "\n",
    "    if \"RAM\" in smartphone and smartphone[\"OS\"][\"name\"]:\n",
    "        os_name = smartphone[\"OS\"][\"name\"]\n",
    "        os_uri = get_or_create_shared_uri(\"os\", os_name)\n",
    "        if os_uri:  # Check if uri is not None\n",
    "            g.add((os_uri, RDF.type, SMP.OS))\n",
    "            g.add((os_uri, SMP.name, Literal(os_name, datatype=XSD.string)))\n",
    "            if smartphone[\"OS\"][\"releaseDate\"]:\n",
    "                os_release_date = format_date(smartphone[\"OS\"][\"releaseDate\"])\n",
    "                if os_release_date:\n",
    "                    g.add((os_uri, SMP.releaseDate, Literal(os_release_date, datatype=XSD.date)))\n",
    "            g.add((product_uri, SMP.hasOS, os_uri))\n",
    "# Serialize to RDF/XML format\n",
    "rdf_data = g.serialize(format='xml')\n",
    "\n",
    "# Save the RDF data to a file\n",
    "with open(\"smartphone.rdf\", \"w\") as f:\n",
    "    f.write(rdf_data)\n",
    "\n",
    "print(\"RDF data has been successfully generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "# Hàm để kiểm tra và thêm dấu ngoặc kép nếu chưa có\n",
    "def add_quotes_if_needed(value):\n",
    "    if value and not (value.startswith('\"') and value.endswith('\"')):\n",
    "        return f'\"{value}\"'\n",
    "    return value\n",
    "\n",
    "# Hàm để chuyển RDF XML thành CSV\n",
    "def rdf_to_csv(rdf_file, csv_file):\n",
    "    # Parse XML file\n",
    "    tree = ET.parse(rdf_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Danh sách các smartphone (mỗi smartphone là một dòng trong CSV)\n",
    "    data = []\n",
    "\n",
    "    # Duyệt qua từng thẻ SmartPhone\n",
    "    for smartphone in root.findall('{http://www.semanticweb.org/hust/master/2024/10/ontologies/smartphone.owl/}SmartPhone'):\n",
    "        row = {}\n",
    "        # Lấy URI từ thuộc tính rdf:about\n",
    "        row['uri'] = smartphone.attrib.get('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}about', '')\n",
    "\n",
    "        # Duyệt qua các thuộc tính con của smartphone\n",
    "        for child in smartphone:\n",
    "            # Lấy tên thẻ và giá trị\n",
    "            tag = child.tag.split('}')[-1]  # Lấy phần sau dấu ngoặc nhọn nếu có namespace\n",
    "\n",
    "            # Nếu thẻ con có thêm các thẻ con khác (ví dụ: Screen, CPU)\n",
    "            if len(child):\n",
    "                for sub_child in child:\n",
    "                    sub_tag = sub_child.tag.split('}')[-1]\n",
    "                    row[sub_tag] = sub_child.text\n",
    "            else:\n",
    "                row[tag] = child.text\n",
    "\n",
    "        # Thêm dòng dữ liệu vào danh sách\n",
    "        data.append(row)\n",
    "\n",
    "    # Lấy các tên cột mà không thêm cột 'uri' lần thứ hai\n",
    "    columns = ['uri'] + [key for key in data[0] if key != 'uri']\n",
    "\n",
    "    # Ghi ra file CSV\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Ghi tiêu đề cột\n",
    "        writer.writerow(columns)\n",
    "\n",
    "        # Ghi từng dòng dữ liệu, bọc giá trị trong dấu ngoặc kép nếu cần\n",
    "        for row in data:\n",
    "            writer.writerow([add_quotes_if_needed(row.get(col, '')) for col in columns])\n",
    "\n",
    "# Gọi hàm để chuyển đổi file RDF thành CSV\n",
    "rdf_to_csv('output.rdf', 'output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'smartphone.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "import csv\n",
    "\n",
    "# Define the namespace for your ontology\n",
    "SMP = rdflib.Namespace(\"http://www.semanticweb.org/hust/master/2024/10/ontologies/smartphone.owl#\")\n",
    "\n",
    "# Load the RDF/XML file\n",
    "rdf_file = 'smartphone.rdf'\n",
    "g = rdflib.Graph()\n",
    "g.parse(rdf_file, format='xml')\n",
    "\n",
    "# List to store rows (uri and product_name)\n",
    "rows = []\n",
    "\n",
    "# Query for RDF descriptions of SmartPhone\n",
    "for subject, predicate, obj in g.triples((None, rdflib.RDF.type, SMP.SmartPhone)):\n",
    "    # Extract the URI (subject) and product name\n",
    "    product_uri = subject\n",
    "    product_name = None\n",
    "\n",
    "    # Look for the product name in the triples\n",
    "    for _, p, o in g.triples((subject, SMP.name, None)):\n",
    "        product_name = str(o)\n",
    "\n",
    "    # Append the URI and product name to the list\n",
    "    if product_name:\n",
    "        rows.append([product_uri, product_name])\n",
    "\n",
    "# Write to CSV\n",
    "csv_file = 'smartphone.csv'\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['uri', 'product_name'])  # Write header\n",
    "    writer.writerows(rows)  # Write data\n",
    "\n",
    "print(f\"CSV file '{csv_file}' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF/XML syntax is valid.\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Load the RDF/XML file\n",
    "g = Graph()\n",
    "try:\n",
    "    g.parse(\"D:/study/SematicWeb/crawl_mobile/smartphone.owl.xml\", format=\"xml\")\n",
    "    print(\"RDF/XML syntax is valid.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in RDF/XML file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Kiểm tra xem các thuộc tính trong RDF có xuất hiện trong ontology hay không\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, p, o \u001b[38;5;129;01min\u001b[39;00m g_data:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg_ontology\u001b[49m:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThuộc tính \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m không có trong ontology\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\study\\SematicWeb\\crawl_mobile\\.venv\\Lib\\site-packages\\rdflib\\graph.py:687\u001b[0m, in \u001b[0;36mGraph.__contains__\u001b[1;34m(self, triple)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, triple: _TripleSelectorType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    686\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Support for 'triple in graph' syntax\"\"\"\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtriple\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriple\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\SematicWeb\\crawl_mobile\\.venv\\Lib\\site-packages\\rdflib\\graph.py:593\u001b[0m, in \u001b[0;36mGraph.triples\u001b[1;34m(self, triple)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtriples\u001b[39m(\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    586\u001b[0m     triple: _TripleSelectorType,\n\u001b[0;32m    587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[_TripleOrTriplePathType, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    588\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generator over the triple store\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \n\u001b[0;32m    590\u001b[0m \u001b[38;5;124;03m    Returns triples that match the given triple pattern. If triple pattern\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    does not provide a context, all contexts will be searched.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m     s, p, o \u001b[38;5;241m=\u001b[39m triple\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, Path):\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _s, _o \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;28mself\u001b[39m, s, o):\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace\n",
    "\n",
    "# Load ontology và RDF/XML\n",
    "ontology_file = './smartphone.owl.xml'\n",
    "data_file = './output_v2.rdf'\n",
    "\n",
    "g_ontology = Graph()\n",
    "g_data = Graph()\n",
    "\n",
    "# Parse các file\n",
    "g_ontology.parse(ontology_file)\n",
    "g_data.parse(data_file)\n",
    "\n",
    "# Namespace của ontology\n",
    "n = Namespace('http://www.semanticweb.org/hust/master/2024/10/ontologies/smartphone.owl/')\n",
    "\n",
    "# Kiểm tra xem các thuộc tính trong RDF có xuất hiện trong ontology hay không\n",
    "for s, p, o in g_data:\n",
    "    if p not in g_ontology:\n",
    "        print(f\"Thuộc tính {p} không có trong ontology\")\n",
    "    else:\n",
    "        print(f\"Thuộc tính {p} hợp lệ\")\n",
    "\n",
    "print(\"Kiểm tra hoàn tất!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
